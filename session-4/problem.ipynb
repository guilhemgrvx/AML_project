{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qElmauNDuHeu"
      },
      "outputs": [],
      "source": [
        "# 1. Self-attention by hand\n",
        "# 2. Self-attention block in pytorch\n",
        "# 3. GPT, piece-by-piece\n",
        "# 4. GPU goes rrrr!\n",
        "\n",
        "# Original code from https://github.com/karpathy/minGPT/tree/master/mingpt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otqjelZ-uHew"
      },
      "source": [
        "### Step 1: Self-attention by hand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "AvLPF07iuHex"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMwC2kBQuHey"
      },
      "outputs": [],
      "source": [
        "#  -- Write the scaled dot product self attention\n",
        "  # 1. Compute queries, keys, and values\n",
        "  # 2. Compute dot products\n",
        "  # 3. Scale the dot products\n",
        "  # 4. Apply softmax to calculate attentions\n",
        "  # 5. Weight values by attentions\n",
        "  # 6. Compute attention weighted features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "oGf5Iag_uHey"
      },
      "outputs": [],
      "source": [
        "# Choose random values for the parameters -- sames values as on slide 12, but in pytorch format\n",
        "# T = 4, C = 6, H = 3\n",
        "X = torch.tensor([[2,0,0,0,2,1],[0,1,2,0,0,0],[0,0,1,1,0,1],[2,0,0,1,0,1]], dtype=float) # T x C\n",
        "W_QT = torch.tensor([[1,0,0], [1,1,0], [0,0,1], [0,1,0], [0,0,1], [0,0,1]], dtype=float) # C x H\n",
        "W_KT = torch.tensor([[0,0,1], [0,1,0], [1,0,0], [0,0,0], [0,0,0], [0,0,-1]], dtype=float) # C x H\n",
        "W_VT = torch.tensor([[10,0,0], [0,0,10], [0,0,0], [0,10,0], [0,0,0], [0,0,0]], dtype=float) # C x H"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1IoASV5uHey"
      },
      "outputs": [],
      "source": [
        "# What does the second dimension of matrices Q and K correspond to?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "HSrhoLTLuHey"
      },
      "outputs": [],
      "source": [
        "# compute the weighted attention matrix S\n",
        "Q = torch.matmul(X, W_QT)\n",
        "K = torch.matmul(X, W_KT)\n",
        "QKT = torch.matmul(Q, K.T)\n",
        "d_k = Q.shape[1]\n",
        "QKT /= math.sqrt(d_k)\n",
        "S = F.softmax(QKT, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ZuKTqjL6uHez"
      },
      "outputs": [],
      "source": [
        "# compute the self-attention matrix A\n",
        "V = torch.matmul(X, W_VT)\n",
        "A = torch.matmul(S,V)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfI1CzRWuHez",
        "outputId": "141149e6-b3bc-4cb7-e402-9ac5b3083909"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "# Sanity check. This should return True.\n",
        "torch.allclose(A.float(), torch.tensor([[10.30759701,  2.83283874,  4.59026201],\n",
        "        [10.10551833,  2.97334971,  4.50027071],\n",
        "        [15.03361159,  4.13169018,  2.10990693],\n",
        "        [ 3.06082018,  1.53041009,  7.70438486]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRnC4Lg7uHez"
      },
      "source": [
        "### Step 2: Self-attention block in pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "nTsFZjcouHez"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.functional import F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "SVkSFHGsuHez"
      },
      "outputs": [],
      "source": [
        "# do not modify this code\n",
        "\n",
        "batch_size = 3 # B\n",
        "block_size = 2 # T\n",
        "n_embd = 3     # C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "19ZgNIfguHez"
      },
      "outputs": [],
      "source": [
        "torch.set_printoptions(precision=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "o715G-H0uHez"
      },
      "outputs": [],
      "source": [
        "# Build a scaled self-attention head without masked attention and without dropout (i.e. just key, query and values)\n",
        "# A matrix multiplication is implemented using the nn.Linear() operator with no bias.\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "\n",
        "    def forward (self, x):\n",
        "        B, T, C = x.shape\n",
        "        K = self.key(x)\n",
        "        Q = self.query(x)\n",
        "        V = self.value(x)\n",
        "        wei = (Q @ K.transpose(-2, -1)) * (K.shape[-1] ** -0.5)\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        out = wei @ V\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wXxfW30uHe0",
        "outputId": "07e43dcc-2882-42b9-ee11-265a05e67e19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.46728206,  0.03477209],\n",
              "         [-0.47425330,  0.05069543]],\n",
              "\n",
              "        [[-0.38198256,  0.02403206],\n",
              "         [-0.39846635,  0.02506738]],\n",
              "\n",
              "        [[-0.29631630,  0.12201241],\n",
              "         [-0.30199534,  0.12650710]]], grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# Unit test. Do not modify this code\n",
        "torch.manual_seed(123) # do not remove this line\n",
        "h = Head(2)\n",
        "torch.manual_seed(123) # do not remove this line\n",
        "x = torch.rand((batch_size, block_size, n_embd))\n",
        "out = h(x)\n",
        "out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EQL4yTQuHe0",
        "outputId": "bb64dca7-f496-4fd0-ccb5-a32d875763dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "# Sanity check. This should return True.\n",
        "torch.allclose(out, torch.tensor([[[-0.46728206,  0.03477207],\n",
        "         [-0.47425330,  0.05069541]],\n",
        "        [[-0.38198256,  0.02403205],\n",
        "         [-0.39846635,  0.02506737]],\n",
        "        [[-0.29631630,  0.12201238],\n",
        "         [-0.30199534,  0.12650707]]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "cno-LIDJuHe0"
      },
      "outputs": [],
      "source": [
        "# Add weighted masked attention and dropout. Dropout comes after the softmax and before the multiplication with the value matrix.\n",
        "# Copy the Head class from the previous exercise and expand upon it.\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # store a persistent buffer for the forward pass\n",
        "\n",
        "    def forward (self, x):\n",
        "        B, T, C = x.shape\n",
        "        K = self.key(x)\n",
        "        Q = self.query(x)\n",
        "        V = self.value(x)\n",
        "\n",
        "        wei = (Q @ K.transpose(-2, -1)) * (K.shape[-1] ** -0.5)\n",
        "\n",
        "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        out = wei @ V\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1o9grjhuuHe0",
        "outputId": "23a5a2db-26fa-47ca-c468-2e7eb5f4e7be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.47424769, -0.20746115],\n",
              "         [-0.59281659,  0.06336929]],\n",
              "\n",
              "        [[-0.17730206,  0.01118640],\n",
              "         [-0.49808297,  0.03133423]],\n",
              "\n",
              "        [[-0.21627384,  0.03053588],\n",
              "         [-0.37749419,  0.15813388]]], grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "# Unit test. Do not modify this code\n",
        "torch.manual_seed(123) # do not remove this line\n",
        "h = Head(2)\n",
        "torch.manual_seed(123) # do not remove this line\n",
        "x = torch.rand((batch_size, block_size, n_embd))\n",
        "out = h(x)\n",
        "out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsIZIBGEuHe0",
        "outputId": "4961ba96-0e09-44af-814a-cfd4a098c3d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# Sanity check. This should return True.\n",
        "torch.allclose(out, torch.tensor([[[-0.37939820, -0.16596894],\n",
        "         [-0.47425330,  0.05069541]],\n",
        "        [[-0.14184165,  0.00894911],\n",
        "         [-0.39846635,  0.02506737]],\n",
        "        [[-0.17301908,  0.02442869],\n",
        "         [-0.30199534,  0.12650707]]]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "XNH1zoYZuHe0"
      },
      "outputs": [],
      "source": [
        "# A multi-head attention module contains a list of heads and a linear projection layer.\n",
        "# The heads are applied to the input and then concatenated along the last dimension, then\n",
        "# the linear layer is applied. Look at the unit test below to determine the dimensions of\n",
        "# the linear layer.\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(num_heads * head_size, n_embd)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "eqQgZjBNuHe1"
      },
      "outputs": [],
      "source": [
        "# do not modify\n",
        "num_heads = 3\n",
        "head_size = 2\n",
        "n_embd = 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "wBdcwehZuHe1"
      },
      "outputs": [],
      "source": [
        "# Unit test. Do not modify this code\n",
        "torch.manual_seed(123) # do not remove this line\n",
        "sa = MultiHeadAttention(num_heads=3, head_size=head_size)\n",
        "torch.manual_seed(123) # do not remove this line\n",
        "x = torch.rand((batch_size, block_size, n_embd))\n",
        "out = sa(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5_Peui3uHe1",
        "outputId": "be88d22a-429f-43c6-f19f-3fea7f8c37c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "# Sanity check. This should return True.\n",
        "torch.allclose(out, torch.tensor([[[-0.03730504, -0.07006130, -0.27096999,  0.13144857, -0.45049590,\n",
        "          -0.33217290],\n",
        "         [-0.06818272, -0.04490501, -0.34806073,  0.15622401, -0.45459983,\n",
        "          -0.33084857]],\n",
        "        [[-0.08914752, -0.03846309, -0.36569631,  0.09802882, -0.39963537,\n",
        "          -0.29225215],\n",
        "         [-0.04541985,  0.01269679, -0.25225419,  0.08241771, -0.41533324,\n",
        "          -0.30674040]],\n",
        "        [[ 0.15234883, -0.08591781, -0.10099770,  0.19886394, -0.49236685,\n",
        "          -0.43605998],\n",
        "         [ 0.15418015, -0.01837257, -0.00573672,  0.14228639, -0.48172480,\n",
        "          -0.40757987]]]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "NyKK2jI7uHe1"
      },
      "outputs": [],
      "source": [
        "# Add a classical feedforward module: linear -> ReLU -> linear\n",
        "# The hidden dimension is four times bigger than the input dimension (see Section 3.3 of Attention is All You Need)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        hidden_dim = 4 * n_embd\n",
        "\n",
        "        self.linear1 = nn.Linear(n_embd, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.linear2 = nn.Linear(hidden_dim, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.linear2(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdRz9FG6uHe1",
        "outputId": "9bfe0a90-e6d8-4015-f032-5a892a16050c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.56171656,  0.00671528, -0.08733369,  0.25574088, -0.21769133,\n",
              "         -0.10549456],\n",
              "        [-0.55107427,  0.11361884, -0.03093255,  0.03408349, -0.33449104,\n",
              "          0.08606286],\n",
              "        [-0.49061197,  0.12174689, -0.05031921, -0.09972109, -0.29760227,\n",
              "         -0.03681775]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "# Unit test. Do not modify this code\n",
        "torch.manual_seed(123) # do not remove this line\n",
        "ff = FeedForward(n_embd)\n",
        "torch.manual_seed(123) # do not remove this line\n",
        "x = torch.rand((3,n_embd))\n",
        "out = ff(x)\n",
        "out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Hu2CPxRuHe1",
        "outputId": "c2dfb034-d78f-4cfd-c018-8500165bad2e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "# Sanity check. This should return True.\n",
        "torch.allclose(out, torch.tensor([[-0.58034140,  0.04641046, -0.10707694,  0.21581653, -0.30361831,\n",
        "         -0.07352637],\n",
        "        [-0.48917407,  0.07879593, -0.15972012,  0.17862344, -0.37070659,\n",
        "         -0.07852858],\n",
        "        [-0.48530388,  0.09604470, -0.06524836,  0.16611034, -0.35499069,\n",
        "         -0.08964306]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "xbNcNw4suHe1"
      },
      "outputs": [],
      "source": [
        "# Build a self-attention block\n",
        "#\n",
        "#   in -----> LayerNorm -------> multi-head attention -- + ----> LayerNorm -----> FeedForward --- + -----> out\n",
        "#         |                                              |   |                                    |\n",
        "#          ----------------------------------------------     ------------------------------------\n",
        "#\n",
        "# This architecture is slightly different from Attention is All You Need (or the UDL textbook):\n",
        "# the layer norm comes before (not after) the attention or feed-forward\n",
        "#\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.attn = MultiHeadAttention(n_head, head_size)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out = self.attn(self.ln1(x))\n",
        "        out = attn_out + x\n",
        "        ffwd_out = self.ffwd(self.ln2(out))\n",
        "        out = ffwd_out + out\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV1pAKfhuHe2",
        "outputId": "ae8a1997-8250-4efc-e867-deb827f67e28"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.28274745, -0.15197974, -0.25662702,  0.96722639, -0.61615402,\n",
              "           0.46517178],\n",
              "         [-0.28723764, -0.49662346, -0.33363408,  1.27716315, -0.37670344,\n",
              "           0.45305294]],\n",
              "\n",
              "        [[-0.61567461, -0.26879269, -0.29994249,  0.59354782, -0.49588484,\n",
              "           0.40434441],\n",
              "         [-0.09587546,  0.02934551,  0.61873144,  0.71886432, -0.02091005,\n",
              "           0.68256164]],\n",
              "\n",
              "        [[ 0.41833729,  0.46945339,  0.02747524,  1.36433983,  0.26742882,\n",
              "          -0.05659816],\n",
              "         [ 1.32717097,  0.78645396,  1.17269289,  0.45673659,  0.74732399,\n",
              "           0.44081396]]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "# Unit test. Do not modify this code\n",
        "torch.manual_seed(123) # do not remove this line\n",
        "bk = Block(n_embd, num_heads)\n",
        "torch.manual_seed(123) # do not remove this line\n",
        "x = torch.rand((batch_size,block_size,n_embd))\n",
        "out = bk(x)\n",
        "out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRbEfeRouHe2",
        "outputId": "fb85b818-7d6a-4404-9f2e-d8c406539bf7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "# Sanity check. This should return True.\n",
        "torch.allclose(out, torch.tensor([[[-0.05278997, -0.10863629, -0.09458938,  0.97590691, -0.55101192,\n",
        "           0.57085067],\n",
        "         [-0.16924502, -0.45394337, -0.25217158,  1.10904062, -0.34593600,\n",
        "           0.41432184]],\n",
        "        [[-0.41515028, -0.30126408, -0.11399293,  0.64651299, -0.51579159,\n",
        "           0.57017863],\n",
        "         [-0.02535054,  0.08704096,  0.66524690,  0.69768047,  0.05969021,\n",
        "           0.69993609]],\n",
        "        [[ 0.52881187,  0.34458166,  0.31130391,  1.11564195,  0.37998506,\n",
        "          -0.02971917],\n",
        "         [ 1.38496208,  0.60325992,  0.99346304,  0.38082033,  0.62151432,\n",
        "           0.47973478]]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ij8DB-qpuHe2"
      },
      "outputs": [],
      "source": [
        "## Step 3: Build a mini GPT\n",
        "#\n",
        "# - Start from the gpt-problem.py file\n",
        "# - Add your Head, MultiHeadAttention, FeedForward and Block classes\n",
        "# - Fill in the GPT class (__init__ and forward methods)\n",
        "# - Train the network on CPU\n",
        "# - Train the network on GPU\n",
        "\n",
        "# For __init__, the GPT model parameters are:\n",
        "#   - a token embedding table\n",
        "#   - a positional embedding table\n",
        "#   - a sequence of Blocks\n",
        "#   - a layer norm\n",
        "#   - a linear layer\n",
        "#\n",
        "# For forward(), the model consists in:\n",
        "#   - applying the token embedding table and positional embedding table to the input tensor\n",
        "#   - adding the two together\n",
        "#   - applying the blocks, layer norm and linear layer (in that order)\n",
        "#\n",
        "# The code comes from hyperparameters that should work well on GPU.  On CPU, you\n",
        "# will need to reduce the model size significantly.\n",
        "#\n",
        "# In pytorch, an learnable embedding table is implemented with nn.Embedding(...)\n",
        "#\n",
        "# The token embedding table learns an embedding for each item of the vocabulary. The\n",
        "# positional embedding table does not depend on the input and learns an embedding\n",
        "# for each position in the context."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}